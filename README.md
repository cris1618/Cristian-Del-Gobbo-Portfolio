# Cristian Del Gobbo Portfolio

## About Me
I'm a curious student who’s always excited to learn something new. I'm currently pursuing a triple major in Mathematics, Data Science & Computer Science with a minor in Business Administration. My academic and professional journey has equipped me with a strong foundation in analytical thinking, problem-solving, and technical expertise.

## Education
### Lyon College
- Triple Major: Mathematics, Data Science and Computer Science.
- Minor: Business Administration.
- Expected Graduation: May 2026.
- GPA: 4.0

## Research and Publications
### Automated Assessment of Artifacts using AI (Published as First Author)
**Description:**
This research investigates the comparative effectiveness of grades and feedback provided by human professors versus those generated by a custom Generative AI (GAI) model, specifically a custom GPT model developed by OpenAI. The study focuses on the quality, accuracy, and usefulness of feedback, as well as the consistency and reliability of grading in an educational context.

**Key Components:**
- *Synthetic Artifact Creation*: Developed synthetic student artifacts using advanced prompt-engineering techniques with GPT-4o, ensuring diversity and realism in the simulated student work.
  
- *Grading Process*: Utilized a custom grader-GPT to evaluate the synthetic artifacts, comparing the results with grades and feedback provided by experienced human professors.
  
- *Statistical Analysis*: Employed Intraclass Correlation Coefficient (ICC) for reliability assessment, Pearson correlation for linear comparisons, and ANOVA to identify significant differences in grading patterns. Paired t-tests were used to compare feedback ratings.
  
- *Evaluation*: Engaged additional professors to assess the accuracy and usefulness of feedback from both human graders and the grader-GPT, ensuring a comprehensive evaluation of the AI's performance in educational assessment.
  
**Significance:**
This research aims to enhance the understanding of AI's role in educational assessment, providing insights into the potential for AI-driven tools to improve the efficiency and quality of grading and feedback processes. The findings have implications for integrating AI in educational settings, ultimately contributing to better learning outcomes and reduced workload for educators.

*Distribution of grades*
![Project 2 Visualization](Asset/AAA-AI-stats.png)

**Link for the full paper:**
[Full Paper](https://www.researchgate.net/publication/386381082_AUTOMATED_ASSESSMENT_OF_ARTEFACTS_USING_AI)

**Code for Statistical Analysis:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/AAA-AI%20Research/AAA-AI%20stats.ipynb)

### Plankton Carbon decay analysis through deep learning models (Not published Yet) 
**Description:**
This research project, conducted in collaboration with one of my professors and international researchers, focuses on developing a deep learning model to detect and classify plankton species from microscopic images. The objective is to automate the process of identifying various plankton types to support marine biology research and environmental monitoring. (Details in Projects section)

**Significance:**
This project demonstrates the transformative potential of deep learning in marine biology by automating the identification of plankton species, a task traditionally reliant on manual labor and expertise. The ability to accurately and efficiently classify plankton from microscopic images supports large-scale ecological studies and environmental monitoring efforts. By leveraging advanced machine learning models and international collaboration, this research contributes to the broader understanding of marine ecosystems and aids in the development of sustainable environmental practices.

*Predictions vs target boxes and label (With contours)*
![Project 2 Visualization](Asset/Plankton_predict.png)

**The Code is private for the moment.** 

### Mapping the use of GAI in a higher educational context (Published as First Author)
**Description:**
This project explores the usage patterns, frequency, and purposes of Generative AI (GAI) tools among students at a small liberal arts college, focusing on a college population with fewer than 1,000 students. Conducted through a survey at Lyon College, the research investigates students’ engagement with GAI, including preferred tools, academic tasks supported by AI, and usage differences across academic disciplines and between international and domestic students.

**Key Components:**
- *Survey Design*: Developed a concise survey with targeted questions on AI awareness, frequency of use, specific academic purposes, and preferred AI tools to capture a snapshot of student engagement with GAI. The survey was designed to maximize student participation while gathering relevant data for analysis.

- *Data Processing and Cleaning*: Organized and preprocessed survey responses to create a structured dataset. This included renaming columns for clarity, managing missing values, and filtering responses based on consent to participate.

- *Exploratory Data Analysis*: Conducted visual analyses to identify trends in AI usage frequency and purposes among different student demographics. Utilized bar charts, stacked plots, and contingency tables to illustrate usage patterns by discipline and student type (domestic vs. international).

- *Statistical Testing*: Performed Chi-square tests to analyze potential associations between discipline and AI usage, and between student type and AI usage purposes. Mann-Whitney U tests were conducted to compare usage frequency differences between international and domestic students.

**Significance:**
This research provides valuable insights into how GAI is integrated into academic practices at a small institution, contributing to the understanding of AI’s role in higher education. The findings suggest high adoption rates of GAI among students, with AI widely used for language support, idea generation, and academic writing assistance. The project highlights the potential for AI to support diverse student needs, including those of international students, and serves as a foundation for further research on AI's impact in educational environments.

*Usage Purposes by Student Type*
![Project 2 Visualization](Asset/Percentage_by_student_type.png)

**Code for Data and statistical Analysis:**
[View the code on GitHub](https://github.com/cris1618/Mapping-the-use-of-GAI-in-a-higher-educational-context/blob/main/AI_usage.ipynb)

**Link for the full paper:**
[Full Paper](https://www.researchgate.net/publication/390207658_GENERATIVE_AI_TOOLS_IN_HIGHER_EDUCATION_A_CASE_STUDY_OF_STUDENT_USAGE_AT_A_SMALL_LIBERAL_ARTS_COLLEGE)

### Synthetic Data Generation Open-source Python Libraries Comparison: SDV and Synthicity (Publication process, First Author)
**Description:**
This research compares the performance of two widely used open-source Python packages for tabular synthetic data generation: SDV and Synthicity. The study evaluates six models (three from each package) in terms of their ability to generate statistically and predictively accurate synthetic datasets from a real-world energy consumption dataset. The goal is to assess each package’s effectiveness in low-data and high-expansion scenarios.

**Key Components:**

- *Dataset Selection:* Used a publicly available dataset from the UCI repository, containing energy usage and environmental features. Only 1,000 real rows were used to simulate low-data conditions and test synthetic data scalability.

- *Synthetic Generation Settings:* Conducted two experiments: a 1:1 experiment (models generate 1,000 rows from 1,000 real rows) and a 1:10 experiment (models generate 10,000 synthetic rows from 1,000 real rows).

- *Statistical Evaluation:* Developed a custom scoring function that compares real and synthetic data column-by-column using summary statistics (mean, median, mode, std), and distributional tests (Kolmogorov–Smirnov, Wasserstein distance). Scores ranged from 0 to 100 for interpretability.

- *Predictive Utility Assessment:* Applied a Train-on-Synthetic, Test-on-Real (TSTR) framework using four regression models (XGBRegressor, Random Forest, SVR, and Linear Regression). A custom performance score was computed by comparing the synthetic-trained model’s MAE, MSE, and R² against real-trained models.

**Significance:**
This project provides an in-depth evaluation of how different synthetic data models perform under constrained data settings. Results show that statistical models (e.g., Bayesian Network, Gaussian Copula) perform better when data is scarce (1:1), while deep learning models (like TVAE) generalize better when asked to generate more data (1:10). Additionally, it highlights that implementations of the same model (e.g., CTGAN, TVAE) can vary significantly across libraries, and that SDV offers better usability and documentation for applied research scenarios.

*Synthetic Data Generators Predictive Utility (1:1 Experiment)*
![Research 4 Visualization](Asset/1_1_performances.png)

**Code for Data and Analysis:**
[View the code on GitHub](https://github.com/cris1618/syntheticData/blob/main/generate_synthetic_data.ipynb)

**Link to ArXiv paper:**
[ArXiv DOI](https://doi.org/10.48550/arXiv.2506.17847)

### KAN-CTGAN: A Kolmogorov–Arnold Network–Enhanced Conditional Tabular Generative Adversarial Architecture (Undergaduate Thesis, in progress)

**IN PROGRESS...**
[GitHub Repository](https://github.com/cris1618/KAN_CTGAN)

### Quantitative study of microbe-interaction with micro- and nano-structures (INBRE 2025 Research Fellowship, in progress)
**General Description (a more detailed description will be given at the end of the program):**
The Wang Laboratory aims to develop and use innovative biophysical tools and advanced imaging tools to solve biomedical and environmental problems by understanding the interactions of microbes (e.g., bacteria) with various structures at micro and nano scales. Quantitative data analysis based on computer vision, machine learning, and deep learning algorithms, as well as computational modeling are also exploited. Projects include, but not limited to, (1) cellular and molecular interactions of bacteria with metal nanoparticles and nanowires, (2) navigation of microbes through porous media and micro-scale mazes, and (3) controlling of microbes with electromagnetic fields.

**IN PROGRESS...**

### Graph-Driven Maze Navigation with a Raspberry Pi Boe-Bot: A Simulation-to-Hardware Approach (In progress)

**IN PROGRESS...**
[GitHub Repository](https://github.com/cris1618/MazeBot)


## Projects
*Note:* Projects are ranked by importance and difficulty (Not exactly, but it's a good rule of thumb). Therefore, the most relevant and challenging projects are listed within the first 10 positions.

### EcoWatt App: Consumption Forecast & Advice (Python, React, JavaScript, CSS, HTML)
**Description:**
In this project, I developed EcoWatt App, a full-stack web application that predicts a household’s monthly energy consumption and delivers personalized, AI-powered suggestions to reduce usage and save money. Built with a React frontend and FastAPI backend, the app guides users through a simple multi-step input wizard, runs real-time predictions via an XGBoost model, and then leverages the OpenAI API to generate three concise, actionable energy-saving tips—complete with a rough dollar-savings estimate. I’m currently collaborating with students from the business department to prepare the app for commercialization, defining go-to-market strategies and conducting pilot user studies.

**Key Features:**
- *Accurate Consumption Forecasts:* Uses a pretrained XGBoost model to estimate monthly kWh usage based on home size, occupants, season, heating, and cooling types.

- *AI-Driven Recommendations:* Integrates with the OpenAI ChatGPT API to generate tailored saving tips and cost-savings estimates.

- *Interactive Data Wizard:* A multi-step React wizard with image preloading and crossfade transitions for a smooth, engaging user experience.

- *Responsive & Accessible UI:* Designed with mobile-first principles and WCAG-compliant styling for broad audience reach.

- *CORS-Enabled FastAPI Backend:* Securely handles prediction and suggestion endpoints, ready for containerized deployment.

**Technologies Used:**
- *Frontend:* React, react-step-wizard, CSS Modules
- *Backend:* FastAPI, Uvicorn, Pydantic, CORS Middleware
- *ML & AI:* Python, XGBoost, Scikit-Learn, MinMaxScaler, OpenAI GPT-3.5-Turbo
- *DevOps & Deployment:* Docker (in progress), environment-based configuration, CI/CD readiness

**Project Outcomes:**
- *High Prediction Accuracy:* Achieved <5% error on test sets for monthly consumption forecasts.
- *Commercialization Roadmap:* Defined pricing tiers, partnership channels, and marketing materials in collaboration with business students.

**Learnings:**
- Mastered end-to-end integration of ML models in production-style APIs.
- Optimized frontend performance through asset preloading and GPU-accelerated CSS transitions.
- Gained hands-on experience working cross-functionally on product commercialization and market validation.
- Refined best practices for environment-based configuration and secure CORS setup.

**GitHub Repository:**
*NOTE*: The code in this repository hasn’t been updated since integrating the OpenAI API and therefore 
no longer reflects the latest version of the application. Consequently, it is significantly behind the 
final release.
[View the code on GitHub](https://github.com/cris1618/Electric_Consumption_Model)

### GPT-2 Local Replication (Python)
**Description:**
In this project, I replicated a GPT-2 model following the detailed instructions provided by Andrej Karpathy in one of his [tutorial videos](https://youtu.be/l8pRSuU81PU?si=aH1r8OYISL3pokbB). The primary objective was to understand and implement a GPT-2 model locally, focusing on text generation tasks.

The project closely mirrors the structure and implementation found in [Karpathy's GitHub repository](https://github.com/karpathy/build-nanogpt), ensuring a comprehensive grasp of the underlying mechanics of GPT-2.

**Key Features:**
- Implemented a GPT-2 architecture: Followed Andrej Karpathy's tutorial to build a robust transformer model.
- Used TikToken for encoding: Leveraged the TikToken encoding method for efficient and accurate tokenization of the input text.
- Distributed Data Parallel (DDP): Utilized DDP to optimize the training process across multiple GPUs.
- Flash Attention Mechanism: Implemented Flash Self-Attention to enhance the efficiency of attention computation.
- Adaptive Learning Rate: Devised a learning rate schedule with linear warmup and cosine decay to ensure optimal training performance.
  
**Technologies Used:**
- Python
- PyTorch
- TikToken
- Distributed Data Parallel (DDP)
- Transformer Model (GPT-2)

**Project Outcomes:**
- Successfully replicated a GPT-2 model capable of generating coherent and contextually relevant text.
- Validated the effectiveness of GPT-2 models for natural language processing tasks.
- Demonstrated the practical implementation of advanced tokenization and attention mechanisms.

**Learnings:**
- Acquired in-depth knowledge of GPT-2 model architecture and its implementation.
- Enhanced understanding of tokenization methods and their impact on model performance.
- Improved skills in optimizing machine learning models for distributed environments.

**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/GPT2-Replication/train_GPT-2.py)

### NanoGPT (Python)
**Description:**
In this project, I trained a decoder-only transformer model following the principles outlined in the seminal paper "Attention Is All You Need" by Vaswani et al. The goal was to generate coherent and contextually relevant text based on Shakespearean language.

To enhance the precision of the model, I utilized the TikToken encoding method instead of traditional single-character encoding. This approach allowed for a more granular and meaningful representation of the text, improving the overall quality of the generated content.

**Key Features:**
- Implemented a transformer architecture focusing solely on the decoder mechanism for text generation.
- Leveraged TikToken for more precise tokenization and encoding, improving model accuracy.
- Utilized PyTorch for model development, training, and evaluation.
- Optimized hyperparameters to ensure efficient training on a CPU-based environment.

**Technologies Used:**
- Python
- PyTorch
- TikToken
- Transformer Model (Decoder-Only)
- Shakespeare Text Dataset

**Project Outcomes:**
-Successfully trained a language model capable of generating Shakespeare-like text.
-Demonstrated the effectiveness of transformer models for natural language processing tasks.
-Showcased the benefits of using advanced tokenization techniques for improving model precision.

**Learnings:**
-Gained deep insights into the implementation of transformer models.
-Enhanced understanding of tokenization methods and their impact on model performance.
-Improved skills in optimizing machine learning models for resource-constrained environments.

**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/44cc270434af585f9141b71d5ff38e590beb14e1/NanoGPT/ShakeTikToken.py)

### OrbitOCR: Embedded Satellite Digit Recognition (Python)
**Description:**
Collaborating with Dr. Birkenkrahe, I helped deliver a precision OCR pipeline for a client’s space-borne imaging system. The goal was to read mechanical counter digits captured by a satellite and convert them into exact numeric readings. Because this solution runs on a Raspberry Pi Zero W 2 (quad-core ARM, 512 MB RAM), every step—from auto-cropping to final post-processing—was optimized for speed, memory efficiency, and ultra-high accuracy.

**Key Features:**
- *Adaptive Preprocessing Pipeline:* Grayscale conversion, conditional inversion, Gaussian/median filtering, morphological opening, and contrast adjustment to maximize Tesseract accuracy under variable lighting.
- *Heuristic Post-Processing:* Confidence thresholds, digit-level filtering, and position-based corrections (e.g., forcing the first digit from “9” to “2” when statistically justified).
- *Robust CLI Interface & Logging:* Command-line inputs/outputs in CSV format, error handling, and detailed logging for end-to-end traceability.

**Technologies Used:**
- *Languages & Libraries:* Python, Pillow, OpenCV, pytesseract
- *Embedded Platform:* Raspberry Pi Zero W 2 (ARM Cortex-A53, 512 MB)
- *OCR Engine:* Tesseract with custom PSM and whitelist configurations
- *Data Handling:* CSV I/O, multiprocessing, garbage collection optimizations
- *Dev Tools:* VS Code (WSL) Remote-SSH for on-device debugging

**Project Outcomes:**
- Achieved over 88.7 % digit-level accuracy on an initial 1 500-image validation set.
- Reduced per-image processing time to under 150 ms, enabling real-time batch throughput on a Pi Zero W 2.
- Delivered a turnkey CLI tool and documented pipeline, ready for integration into the client’s automated meter-reading system.

**Learnings:**
- Designed and tuned image preprocessing strategies to handle extreme contrasts and sensor noise from orbital imaging.
- Balanced OCR accuracy against memory constraints, applying morphological operations and selective blurring for optimal results.
- Implemented position-based correction heuristics grounded in domain-specific statistics to further boost reliability.
- Gained hands-on experience deploying and profiling Python-based computer vision code on resource-limited hardware.

**Code:**
[View the code on GitHub](https://github.com/cris1618/Image-to-num/tree/main/image-to-number)

### ML From Scratch (Python and C)
**Description:**
I am developing core machine learning algorithms from scratch in both Python and C, with the objective of implementing a comprehensive suite of models without relying on external machine learning libraries. This hands-on approach aims to deepen my understanding of the theoretical foundations and low-level operations that power modern machine learning frameworks.

The project covers essential concepts such as gradient descent, softmax regression, cross-entropy loss, and matrix operations, gradually expanding to include more advanced algorithms like decision trees, support vector machines, and neural networks. By translating implementations between Python and C, I explore the trade-offs between high-level and low-level programming paradigms in machine learning.

**Key Features:**
- Developing a wide range of machine learning algorithms from scratch
- Implementing critical components such as softmax functions, activation functions, loss functions, and gradient descent manually.
- Writing custom data generation, train-test split, and matrix manipulation functions.
- Building efficient C implementations to improve computational performance and memory usage.
- Ensuring numerical stability and scalability by focusing on low-level optimization techniques.

**Technologies Used:**
- Python
- C
- Matplotlib (for Python visualization)
- Linear Algebra and Matrix Operations 
- Gradient Descent Optimization
- Cross-Entropy and MSE Loss Functions

**Project Outcomes (Ongoing):**
- Established a strong foundation for machine learning by creating models from the ground up.
- Bridged the gap between high-level machine learning frameworks and low-level systems programming.
- Created a scalable framework to incrementally add more complex algorithms over time.
- Gained valuable insights into model performance, optimization techniques, and computational efficiency.

**Learnings:**
- Acquired an in-depth understanding of the mathematical principles behind machine learning algorithms.
- Strengthened problem-solving skills through debugging and fine-tuning custom implementations.
- Enhanced my ability to optimize models for performance by working with C.
- Developed transferable skills applicable to real-world machine learning and software engineering projects.

**Code:**
[View the code on GitHub](https://github.com/cris1618/ML_From_Scratch)

### Neural Network in C (PyTorch speed comparison)
**Description:**
I developed and trained a simple neural network (NN) to solve the XOR problem, comparing implementations in both C and Python using the PyTorch framework. The XOR problem is a classic benchmark for NNs, where the goal is to predict the output as true (1) when one input is true and the other is false, and false (0) when both inputs are either true or false.

The neural network architecture consists of two input nodes, three hidden layers with two nodes each, and one binary output node. Originally designed with just one hidden layer, I extended the network to include more hidden layers for experimentation and deeper learning.

**Key Features:**
- Implemented a neural network in both C and Python for a performance comparison.
- C implementation offers fine-grained control and understanding of NN internals, while the Python (PyTorch) version benefits from abstraction and ease of use.
- Forward and backward passes in C were manually coded, highlighting potential for abstraction into a reusable C library, similar to PyTorch.
- The neural network trained to solve the XOR problem over a set number of epochs, and results from both implementations were compared.

**Technologies Used:**
- C
- Python
- PyTorch
- Neural network for XOR problem

**Project Outcomes:**
- Successfully implemented and trained the neural network in both languages.
- Demonstrated the significant speed advantage of C over Python in this context.
- Highlighted the abstraction benefits in Python using frameworks like PyTorch, especially for ease of implementation.

**Learnings:**
- Deepened understanding of neural network architecture and manual implementation in C.
- Gained insights into how high-level frameworks like PyTorch simplify tasks while abstracting essential processes.
- Improved awareness of the trade-offs between control, speed, and ease of use when selecting a programming language for machine learning tasks.

**Code:**
[View the code on GitHub](https://github.com/cris1618/NeuralNetwork_in_C/blob/main/NeuralNetworkC.org)

### VizBeuty: Python Package for Enhanced Visualizations 
**Description:**
VizBeauty is a sophisticated Python toolkit designed to enhance data visualization and analysis. This package offers a comprehensive suite of functions tailored to meet diverse analytical needs with precision and efficiency. By integrating advanced statistical methods and customizable plotting capabilities, VizBeauty facilitates in-depth data exploration and provides valuable insights across various datasets. VizBeauty is publicly available on PyPI, making it easily accessible for all users.

**Tools and Technologies Used:**
- Python: Core programming language for development.
- Pandas: For data manipulation and analysis.
- NumPy: For numerical operations.
- Seaborn: For statistical data visualization.
- Matplotlib: For creating static, animated, and interactive visualizations.
- SciPy: For advanced statistical computations.
- Git: Version control.
  
**Key Insights and Outcomes:**
- Descriptive Statistics: Function to print detailed descriptive statistics, providing insights into the distribution and characteristics of datasets.
  
- Bar Plot with Annotations: Customizable bar plot function with annotations and average value indicators, enhancing data interpretability.
  
- Pearson Correlation: Function to compute and determine the statistical significance of Pearson correlation coefficients, aiding in the identification of relationships between variables.
  
- Scatter Plot with Regression Line: Advanced scatter plot function that includes regression lines and supports additional parameters such as hue and size for deeper analysis.
  
- Hyperparameter Visualization: Function to visualize the impact of hyperparameters on model performance, facilitating more effective model tuning.
  
These functions collectively empower users to perform comprehensive data analysis and visualization, leading to actionable insights and informed decision-making.

*Example of a package function visualization:*
![Project 2 Visualization](Asset/VizBeauty Visualization.png)

**Code:**
[View the code on GitHub](https://github.com/cris1618/VizBeauty-)

### Ocean Vision (Python)
**Description:**
This research project, conducted in collaboration with one of my professors and international researchers, focuses on developing a deep learning model to detect and classify plankton species from microscopic images. The objective is to automate the process of identifying various plankton types to support marine biology research and environmental monitoring.

**Tools and Technologies Used:**
- Programming Languages: Python
- Deep Learning Frameworks: PyTorch, Torchvision
- Models: Faster R-CNN with ResNet-50 backbone
- Data Processing: Pandas, NumPy
- Image Processing: PIL (Python Imaging Library)
- Evaluation Metrics: Mean Average Precision (mAP), Multiclass AUPRC (Average Precision)
- Hardware: GPU for training acceleration
- Additional Libraries: Torcheval for evaluation, Matplotlib for visualizations
  
**Key Insights and Outcomes:**
- Successfully implemented and trained a Faster R-CNN model tailored for plankton detection and classification.
  
- Achieved initial baseline performance metrics, indicating the potential for high-accuracy classification with further training and data augmentation.
  
- Demonstrated the effectiveness of transfer learning by utilizing a pre-trained ResNet-50 backbone, significantly reducing the training time and computational resources required.
  
- Highlighted the importance of data preprocessing and augmentation in enhancing model performance for small and imbalanced datasets.
  
- Provided valuable insights into the application of deep learning techniques in marine biology, paving the way for future research and development in automated species identification.

*Early visualized prediction of the model:*
![Project 3 Visualization](Asset/Predictions.png)

**Code**: 
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/PlanktonResearch/NewTLPreloadedFRCNN.py)

### InsightVisualizer (Python)
**Description:**
In this project, I explore the correlations between various socio-economic indicators such as life expectancy, GDP per capita, and happiness score across different countries using two distinct datasets: "Life expectancy.csv" and "Suicide Rate.csv". Through meticulous analysis, these datasets are merged to construct a unified view of global socio-economic dynamics, uncovering underlying patterns and correlations.

While the project does not draw definitive conclusions due to potential inaccuracies in certain data columns, it leverages visualizations, custom functions, and machine learning techniques to extract valuable insights. Strong correlations are observed between life expectancy, GDP per capita, and happiness score, indicating significant associations among these factors. However, minimal to no discernible relationship is found between other variables and suicide rate, possibly influenced by the nature of the data.

**Tools and Technologies Used:**
- Python: Core programming language for development.
- Pandas: For data manipulation and analysis.
- NumPy: For numerical operations.
- Seaborn: For statistical data visualization.
- Matplotlib: For creating static, animated, and interactive visualizations.
- SciPy: For advanced statistical computations.
- Scikit-learn: For implementing machine learning models.
- XGBoost: For gradient boosting machine learning models.
- Random Forest Regressor: For ensemble learning methods.
  
**Key Insights and Outcomes:**
- Data Merging and Cleaning: Combined two datasets to create a comprehensive view of socio-economic factors.
  
- Descriptive Statistics and Visualizations: Generated detailed statistics and visualizations to understand data distributions and identify outliers.
  
- Correlation Analysis: Discovered strong correlations between life expectancy, GDP per capita, and happiness score, and minimal correlations involving suicide rates.
  
- Machine Learning Models: Developed models to predict life expectancy, GDP per capita, and happiness score, achieving promising performance metrics despite the limited data.
  
- Feature Importance Analysis: Identified key features impacting the prediction models, highlighting the significance of happiness score, fertility rate, and GDP per capita in influencing life expectancy and happiness.

*Heatmap taken from the project*
![Project 4 Visualization](Asset/InsightVisulaizer.png)

**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/InsightVisualizer/Notebook.ipynb) 

### IQ and Income: Unraveling Correlations and Thresholds (Python)
**Description:**
This project focuses on exploring correlations and analyzing the statistical significance of key socio-economic variables. Specifically, it investigates the relationship between intelligence quotient (IQ) and average income, alongside the impact of education expenditure and average temperature on income levels.

**Tools and Technologies Used:**
- Python: Core programming language for development.
- Pandas: For data manipulation and analysis.
- NumPy: For numerical operations.
- Matplotlib: For creating static and interactive visualizations.
- Seaborn: For statistical data visualization.
- SciPy: For advanced statistical computations.

**Key Insights and Outcomes:**
- Positive correlation (0.56) between IQ and average income, suggesting an association between intelligence and income levels.
  
- Stronger correlation (0.86) between education expenditure and average income, highlighting the significant impact of investments in education on income.
  
- Negative correlations (-0.44 between average temperature and average income, and -0.63 between IQ and average temperature), indicating potential influences of
environmental factors on income levels. These correlations underscore associations rather than causal relationships, providing valuable insights into socio-economic dynamics.

*IQ vs Average Income (logarithmic scale)*
![Project 5 Visualization](Asset/IQ.png)

**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/IQ-Income/notebook.ipynb)

### Movie Recommendation Engine (Python)
**Description:**
This project focuses on building a movie recommendation engine using the concept of cosine similarity. The engine suggests movies that are similar to a given movie based on features such as keywords, cast, genres, and director. By utilizing natural language processing (NLP) techniques and machine learning algorithms, the project aims to provide accurate and relevant movie recommendations.

**Tools and Technologies Used:**
- Python: Core programming language for development.
- Pandas: For data manipulation and analysis.
- NumPy: For numerical operations.
- Scikit-learn: For vectorization and similarity calculation.
- Cosine Similarity: A metric to measure the similarity between two vectors.
- CountVectorizer: To convert text data into vectors for comparison.

**Key Insights and Outcomes:**
- The recommendation engine successfully identifies similar movies based on selected features.
- Cosine similarity is effectively utilized to rank movies by their similarity scores.
- By combining features like keywords, cast, genres, and director, the model provides recommendations that align with user preferences.

**Example Recommendation:**
Given the movie "Avatar," the engine suggests similar titles like "Aliens," "Titanic," and "Star Wars," demonstrating its ability to recognize related themes and cast.

**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/MovieRecML/MovieRec.ipynb)

### Mapping the Adoption Patterns of Battery Electric and Plug-in Hybrid Vehicles in Urban, Suburban, and Rural Areas of Washington (Python)
**Description:**
This project analyzes the geographic distribution of Battery Electric Vehicles (BEVs) and Plug-in Hybrid Electric Vehicles (PHEVs) in Washington State. The objective is to determine if there is a significant difference in the adoption patterns of these vehicles across urban, suburban, and rural areas. The hypothesis is that BEVs are more prevalent in urban areas, while PHEVs are more common in suburban and rural regions.

**Tools and Technologies Used:**
- Programming Language: Python
- Pandas: For data manipulation and analysis.
- Matplotlib and Seaborn: For data visualization.
- Scipy: For conducting statistical tests.
  
**Data Sources:**
- Electric Vehicle registrations data.
- Demographic data for Washington State.
  
**Key Insights and Outcomes:**
- Distribution of EVs: The analysis revealed a significant difference in the distribution of BEVs and PHEVs across different area types. BEVs are more prevalent in urban areas, while PHEVs are more common in suburban and rural areas.
  
- Statistical Analysis:
    - Chi-Square Test: Conducted to determine if there is a statistically significant difference in the distribution of BEVs and PHEVs across area types. The chi-square test results indicated a significant difference (χ² = 829.25, p < 0.001).
      
- Proportion Comparison: Compared the proportions of BEVs and PHEVs in urban, suburban, and rural areas to provide insights into their relative frequencies.
  
**Implications for the Field:**
- Infrastructure Planning: Helps in planning and optimizing charging infrastructure to meet the needs of different regions.
- Policy Development: Provides insights for policymakers to design targeted incentives and regulations to encourage EV adoption.
- Market Strategies: Allows automotive companies to tailor their marketing and distribution strategies based on regional preferences for BEVs and PHEVs.
  
**Future Possibilities:**
- Further research on factors driving distribution patterns, such as socio-economic factors and local policies.
- Enhanced data collection for deeper insights.
- Integration with smart grids for energy optimization and grid reliability.

*EV Distribution*
![Project 6 Visualization](Asset/EV.png)

**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/tree/main/EVDistribution)

### Clustering Antarctic Penguin Species (Python)
**Description:**
This project supports a team of researchers collecting data about penguins in Antarctica. The task involved applying data science skills to identify groups in the dataset using K-means clustering. The dataset, provided by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, includes measurements such as culmen length, culmen depth, flipper length, body mass, and penguin sex. The goal was to help classify the penguins into the three native species: Adelie, Chinstrap, and Gentoo.

**Tools and Technologies Used:**
- Python: Data manipulation, analysis, and visualization
- Sklearn: KMeans, PCA, StandardScaler for clustering and data preprocessing
- Matplotlib: Visualization of clustering results
- Pandas: Data handling and manipulation

**Key Insights and Outcomes:**

Source: @allison_horst GitHub

By applying K-means clustering, the project successfully classified the penguins into three distinct clusters, corresponding to the three native species. This classification aids the researchers in understanding the population distribution and characteristics of the penguins in the region.

*Number of Clusters selection*
![Project 7 Visualization](Asset/penguin.png)

**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/AntarticPenguin/notebook.ipynb)

### Predictive Modeling for Agriculture (Python)
**Description:**
This project involves building a machine learning model to help farmers select the best crops for their fields based on essential soil metrics. By analyzing the dataset containing nitrogen, phosphorous, potassium levels, and pH values, the goal is to predict the optimal crop type to maximize yield. The project addresses the challenge of multicollinearity among features to improve model accuracy.

**Tools and Technologies Used:**
- Python: Primary programming language for data analysis and modeling
- Pandas: For data manipulation and preprocessing
- Scikit-learn (Sklearn):
-- Logistic Regression: To build and evaluate the multi-class classification model
-- Train-Test Split: To split the dataset for training and testing
-- F1 Score: To measure model performance
- Seaborn and Matplotlib: For data visualization and creating heatmaps to analyze feature correlations
  
**Key Insights and Outcomes:**
- Successfully trained a logistic regression model to predict crop types based on soil measurements.
- Identified and addressed multicollinearity among features, selecting the best combination of features for the final model.
- Achieved a low weighted F1 score on the test data, demonstrating the model's effectiveness in predicting the optimal crop for different soil conditions.

*Feature selection Heatmap*
![Project Visualization](Asset/Agriculture.png)

**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/AgriculturePred/notebook.ipynb)

### Modeling Car Insurance Claim Outcomes (Python)
**Description:**
This project involves building a machine learning model (Only feature importances) to predict whether a customer will make a claim on their car insurance during the policy period. The model aims to optimize pricing strategies for the insurance company and improve their prediction accuracy. The dataset includes various features such as age, gender, driving experience, education, income, credit score, and vehicle ownership status. The goal is to identify the single feature that results in the best performing model, as measured by accuracy, so they can start with a simple model in production.

**Tools and Technologies Used:**
- Python: General programming and data manipulation
- Pandas: Data manipulation and analysis
- Scikit-learn: Machine learning algorithms and evaluation
- Matplotlib & Seaborn: Data visualization
- Statsmodels: Statistical modeling
  
**Key Insights and Outcomes:**
- Conducted data preprocessing including handling missing values and encoding categorical features.
- Built and evaluated multiple logistic regression models to identify the feature with the highest predictive power.
- Driving experience was identified as the single best feature, achieving an accuracy of 77.71%.
  
**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/CarModel/notebook.ipynb)

### Predicting Movie Rental Durations (Python)
**Description:**
This project focuses on predicting the number of days a customer will rent a DVD using various regression models. The dataset includes features such as rental and return dates, amount paid, rental rate, release year, length, replacement cost, and special features. The objective is to create a model that yields a Mean Squared Error (MSE) of 3 or less on the test set to help the DVD rental company improve inventory planning.

**Tools and Technologies Used:**
- Python: For data manipulation and analysis.
- Pandas: To handle and process the dataset.
- NumPy: For numerical operations.
- Scikit-learn: For model implementation, including Lasso regression, Random Forest, and OLS regression.
- Matplotlib & Seaborn: For data visualization (though not explicitly used in this notebook).
- Jupyter Notebook: For interactive data analysis and model building.

**Key Insights and Outcomes:**
- Conducted feature engineering to create meaningful variables, such as rental length and dummy variables for special features.
  
- Implemented and evaluated multiple regression models including Lasso regression, Linear regression, and Random Forest to predict rental duration.
  
- Performed hyperparameter tuning for Random Forest using RandomizedSearchCV to optimize model performance.
  
- Achieved the lowest MSE with the Random Forest model, indicating it as the best performing model for this task.
  
- Provided actionable insights for the DVD rental company to enhance their inventory planning based on the predictive model.

**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/RentalPred/notebook.ipynb)

### Analyzing Crime in Los Angeles (Python)
**Description:**
This project analyzes crime data in Los Angeles to identify patterns and assist the LAPD in resource allocation.

**Tools and Technologies Used:**
- Python: For data analysis.
- Pandas: For data manipulation.
- Matplotlib and Seaborn: For data visualization.

**Key Insights and Outcomes:**
- Data Cleaning: Loaded and parsed crime data, converting dates and handling time as strings.
  
- Exploratory Data Analysis (EDA): Identified the hour with the highest frequency of crimes. Determined the location with the highest crime rate. Analyzed night crime patterns and identified the most affected area. Categorized victims' ages into brackets and analyzed the distribution.
  
- Visualization: Created count plots to visualize crime occurrences by hour and victim age brackets.

*Number of crime per hour*
![Project Visualization](Asset/LA.png)

**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/LACrime/notebook.ipynb)

### Visualizing the History of Nobel Prizes (Python)
**Description:**
This project focuses on analyzing data related to Nobel Prize winners. The analysis covers various aspects such as the distribution of prizes across different categories, the demographic details of the laureates, and the trends over the years. The primary goal is to uncover patterns and insights about the Nobel Prize awards.

**Tools and Technologies Used:**
- Python: The primary programming language used for data analysis.
- Pandas: For data manipulation and analysis.
- NumPy: For numerical operations.
- Matplotlib and Seaborn: For data visualization.
- Jupyter Notebook: As the development environment for running and presenting the analysis.

**Key Insights and Outcomes:**
- Data Collection
- Exploratory Data Analysis (EDA): Analyzed the distribution of Nobel Prizes across different categories (Chemistry, Literature, Physics, etc.).
- Visualization: Created informative visualizations to highlight key findings, such as bar charts, histograms, and line plots.

*Proportion of Female Winner per Category*
![Project Visualization](Asset/Nobel.png)

**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/NobelPrize/notebook.ipynb)

### Hypotesis Testing with Men's and Women's soccer Matches (Python)
**Description:**
This project analyzes whether more goals are scored in women's international soccer matches compared to men's. The analysis is based on official FIFA World Cup match data since January 1, 2002.

**Tools and Technologies Used:**
- Python: For data analysis.
- Pandas: For data manipulation.
- Matplotlib: For data visualization.
- SciPy: For statistical testing.
  
**Key Insights and Outcomes:**
- Data Collection: Used datasets containing results of official men's and women's FIFA World Cup matches.
  
- Exploratory Data Analysis (EDA): Combined men's and women's datasets for comparative analysis. Calculated the total goals scored in each match.
  
- Statistical Analysis: Conducted a Mann-Whitney U test to compare the number of goals scored in men's and women's matches.
  
- Visualization: Created histograms to visualize the distribution of goals scored in both men's and women's matches.

*Distribution of total goals in World Cup matches*
![Project Visualization](Asset/WorldCup.png)

**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/SoccerAnalysis/notebook.ipynb)

### Exploring NYC Public School test Results Scores (Python)
**Description:**
This project analyzes SAT scores of American high school students to identify trends and insights. The analysis focuses on the scores across different sections and explores factors that might influence performance.

**Tools and Technologies Used:**
- Python: For data analysis.
- Pandas: For data manipulation.

**Key Insights and Outcomes:**
- Data Collection: Utilized datasets containing SAT scores of high school students.
- Exploratory Data Analysis (EDA): Analyzed SAT scores across different sections: literacy, numeracy, and writing.

*SAT Scores in New York's Borough*
![Project Visualization](Asset/SAT.png)

**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/NYCTestScore/notebook.ipynb)

### Dr. Semmelweis and the Importance of Handwashing (R)
**Description:**
This project reanalyzes historical data to examine the impact of handwashing, as discovered by Dr. Ignaz Semmelweis, on reducing childbed fever at the Vienna General Hospital in the 1840s.

**Tools and Technologies Used:**
- R: For data analysis and visualization.
- Tidyverse: For data manipulation and visualization.

**Key Insights and Outcomes:**
- Data Loading: Loaded datasets containing yearly and monthly births and deaths data from two clinics at the Vienna General Hospital.
  
- Data Cleaning and Transformation: Created columns to calculate the proportion of deaths relative to births. Added a column to mark the start of handwashing.
  
- Exploratory Data Analysis (EDA): Visualized the proportion of deaths over time by clinic and overall. Analyzed the impact of handwashing by comparing death proportions before and after its introduction.
  
- Visualization: Plotted the proportion of deaths over time using line graphs. Highlighted the change in mortality rates after handwashing was implemented.

*Handwashing Impact*
![Project Visualization](Asset/Handwashing.png)

**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/CleanData/notebook.ipynb)

### When Was the Golden Era of Video Games? (SQL)
**Description:**
This project analyzes video game critic and user scores, as well as sales data, for the top 400 video games released since 1977. The goal is to identify trends in game quality and sales, and to explore whether there is a "golden age" of video games.

**Tools and Technologies Used:**
- SQL: For data querying and analysis.

**Key Insights and Outcomes:**
- Data Queries: Extracted the top 10 best-selling games. Analyzed the top years by average critic scores. Compared years with high critic and user scores to identify potential golden years.
  
- Analysis:
  - Best-Selling Games: Identified the games with the highest sales numbers.
  - Top Critic Years: Determined which years had the highest average critic scores for games with at least five releases.
  - Golden Years: Found years where the average critic or user score was exceptionally high and compared the differences between critic and user scores.

*Golden Years in Video Games*
![Project Visualization](Asset/VideoGames.png)
 
**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/VideoGamesSQL/notebook.ipynb)

### Analyzing Students' Mental Health (SQL)
**Description:**
This project analyzes mental health data from a survey of international and domestic students at a Japanese international university. The goal is to explore the impact of being an international student on mental health, social connectedness, and acculturative stress.

**Tools and Technologies Used:**
- SQL: For data querying and analysis.

**Key Insights and Outcomes:**
- Data Queries: Extracted counts of undergraduate and graduate students. Compared the number of international and domestic students. Calculated average scores for depression (PHQ-9), social connectedness (SCS), and acculturative stress (ASISS) for both international and domestic students. Analyzed the relationship between the length of stay and mental health scores for international students.
  
- Analysis:
  - Student Demographics: Determined the distribution of students by academic level and origin (international vs. domestic).
  - Mental Health Scores: Found that international students had different average scores for depression, social connectedness, and acculturative stress compared to domestic students.
  - Length of Stay: Investigated how the length of stay in Japan affected mental health indicators for international students.

*Depression average test scores*
![Project Visualization](Asset/MentalHealth.png)
 
**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/MentalHealth/notebook.ipynb)

### Analyzing Motorcycle Part Sales
**Description:**
This project analyzes sales data for a company selling motorcycle parts to understand wholesale revenue by product line, month, and warehouse. The analysis aims to calculate the net revenue from wholesale orders, considering the payment fees associated with different payment methods.

**Tools and Technologies Used:**
- SQL: For data querying and analysis.

**Key Insights and Outcomes:**
- Data Queries:
  - Extracted relevant columns from the sales data, including product line, order date, warehouse, client type, total price, and payment fee.
  - Converted the order dates to corresponding months (June, July, August).
  - Filtered the data to include only wholesale orders.
  - Calculated the net revenue by subtracting payment fees from the total price.
  - Grouped results by product line, month, and warehouse.
    
- Query Output:
  - The query output presents the net revenue for each product line, grouped by month and warehouse, allowing the company to gain insights into their wholesale revenue distribution.

*Revenues per product type*
![Project Visualization](Asset/ProductSales.png)

**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/MotorCycleSQL/notebook.ipynb)

### Customer Analytics: Preparing Data for Modeling (Python)
**Description:**
This project focuses on cleaning and optimizing a large customer dataset for efficient storage and analysis. The dataset is intended for predicting whether students are looking for a new job.

**Tools and Technologies Used:**
- Python: For data cleaning and optimization.
- Pandas: For data manipulation.

**Key Insights and Outcomes:**
- Data Cleaning: Loaded the customer dataset and created a copy for cleaning. Converted data types to more memory-efficient formats (e.g., integers to int32, floats to float16). Categorized columns with predefined orderings for better analysis and storage efficiency.
  
- Optimization: Filtered the dataset to include students with at least 10 years of experience and who work in companies with 1000-4999 employees.

**Code:**
[View the code on GitHub](https://github.com/cris1618/Projects-Code/blob/main/CleanData/notebook.ipynb)
